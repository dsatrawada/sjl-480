{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb16d6b-b21b-45de-945c-40be3379c680",
   "metadata": {},
   "source": [
    "CSC480 Assignment 1 tutorial\n",
    "Damien Trunkey, Emily Lucas, Sophia Parrett, Fernando Valdivia, Divya Satrawada, Rey Ortiz\n",
    "Social Justice League!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40278fc-f8a8-4345-9d14-dad5a1030869",
   "metadata": {},
   "source": [
    "# Tutorial: Preprocessing Text Data Into TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eabe39-5cfb-4b20-98bb-589681a026dc",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "    \n",
    "This is a tutorial on how to preprocess data to make it ready to train a model. \n",
    "\n",
    "You can get data many ways. You can use [webscraping](https://realpython.com/python-web-scraping-practical-introduction/#reader-comments), you can get a dataset from [kaggle datasets](https://www.kaggle.com/datasets), or you can get data using an api that has access to  lots of data. We chose to use the reddit.com api to get data from the reddit.com forums. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9d883-78e7-419d-837b-5902ba75367c",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using pandas and numpy, which are two data science libraries in python. The pandas import alows us to create data frames (matrices) and numpy lets us do many mathematical functions.\n",
    "\n",
    "Numpy was originally created to add array functionality to Python. Numpy currently has many functions to support arrays and mathematical functions commonly used by scientists. We chose Numpy because we were going to be working with matrices and mathematical functions. We specifically use Numpy in this example at the end to take the log of all the values in a DataFrame (matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b96d4e-865d-4b67-9515-31c06f7d3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70034749-18ff-4598-a107-565843045cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into a dataframe to manupulate and do preprocessing\n",
    "dataFile = 'AmItheAsshole_subreddit.csv'\n",
    "df = pd.read_csv(dataFile)\n",
    "df = df.head(7)\n",
    "df = df[['selftext', 'ups']]\n",
    "df = df[2:7]\n",
    "ground_truth = df['ups']\n",
    "print(ground_truth)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2265d11-49c9-4992-9ae5-0a244dd9ea7b",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "In data science, normalization is the process of simplifying text in order to make it easier to work with. This is achieved by multiple steps. To begin with, change all of the words with the lower-case version of themselves using .lower(). This is needed because words such as \"They\" and \"they\" would be counted as two different words. However, these should be counted as the same word, so .lower() is used to normalize the capitalization of the text. \n",
    "\n",
    "After all the words are changed to lower-case, the next step is to replace all non-alphanumeric characters with whitespace. This is achieved by utilizing the .replace(\"[^\\w\\s]\", \" \") method. This would eliminate all punctuation, as well as any additional spaces, tabs, or newlines between words and replace it with whitespace. [^\\w\\s] is a regular expression that is used to capture all non-alphanumeric words. Once again, this is needed because punctuation is irrelevant when it comes to natural language processing and elimination it would further simplify the text. \n",
    "\n",
    "Additionally, replacing everything with whitespaces is needed for the final step, which is to split the text on whitespace. This is achieved by utilizing .split(). This method takes the string of text and turns it into a list of separate words. It does this by going through each letter of the string and anytime a whitespace is found, the previous grouping of letters is taken and inserted into a list as one element. To summarize, the steps of normalization are taking the string and applying .lower(), .replace(\"[^\\w\\s]\", \" \"), and finally utilizing .split(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93b0c41-61b0-4fcb-8677-b226faac6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = (\n",
    "    df['selftext'].\n",
    "    str.lower().                  # convert all letters to lowercase\n",
    "    str.replace(\"[^\\w\\s]\", \" \", regex=True).  # replace non-alphanumeric characters by whitespace\n",
    "    str.split()                   # split on whitespace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf6c80d-ec89-4c5e-8b32-72fc1e6e49c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting raw frequency to turn into tf-idf vectors\n",
    "raw_frequency = bag_of_words.apply(Counter)\n",
    "\n",
    "df['selftext'] = raw_frequency\n",
    "\n",
    "tf = pd.DataFrame(list(raw_frequency),index=raw_frequency.index)\n",
    "columns = list(tf.columns)\n",
    "tf = tf.fillna(0)\n",
    "display(tf)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa71dc-ec72-4998-b237-c29f176a3d62",
   "metadata": {},
   "source": [
    "## Remove Stop Words \n",
    "Next in this tutorial we will remove stop words. Removing stop words is important because it gets rid of trivial words (like \"a\", \"the\" etc.), and focuses on more important information. Words that are kept are more topical, and have a stronger connotation. This can be done by creating a list of stop words (from 'stopwords-short.txt') and removing columns in our dataframe that contain stop words. We have to do this step after normalization because otherwise we would not remove stop words that had different capitalizations from the words in our dataset. \n",
    "\n",
    "For example: it would remove \"in\" but not \"In\" if we did not normalize the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1142a-a37d-46f2-950b-b2e6e425359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopFile = 'stopwords.txt'\n",
    "f = open(stopFile, \"r\")\n",
    "stop_words = []\n",
    "for line in f:\n",
    "    words = line.split(',')\n",
    "for word in words:\n",
    "    word = word.replace('\"', \"\").strip(\" \").lower()\n",
    "    stop_words.append(word)\n",
    "    \n",
    "for col in columns:\n",
    "    if col in stop_words:\n",
    "        tf = tf.drop([col], axis=1)\n",
    "display(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47554ea-b557-470f-a852-72327d975096",
   "metadata": {},
   "source": [
    "## TF-IDF Vectors\n",
    "\n",
    "TF-IDF stands for term frequency-inverse document frequency and it is a measure to quantify  how important certain terms are throughout a list of documents (corpus). \n",
    "Term frequency (TF) is the measure of how often a word appears in a single document and inverse document frequency (IDF) is the measure of how common or uncommon a term is throughout the corpus. If a word appears in only two documents, then it may be considered rare, therefore it carries more importance. The equation to find IDF follows the form where t is the term and D is the corpus and d is the current document and N is the number of documents:\n",
    "idf(t,D) = log(N / count(d D; t  D))\n",
    "IDF is important because it takes common words in the English language and weights them less, giving less common words more impact.\n",
    "When we put TF and IDF together we can show that a term is inversely related to its frequency across documents. By multiplying these together we can get the final TF-IDF value. Higher values means a term holds more importance and closer to 0 means it's less relevant.\n",
    "ifidf(t,d,D) = tf(t,d) * idf(t,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b5276-b5d2-4e79-a616-cd673ac8d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document frequencies \n",
    "# (How many documents does each word appear in?)\n",
    "df = (tf > 0).sum(axis=0)\n",
    "\n",
    "# Get IDFs\n",
    "idf = np.log(len(tf) / df)\n",
    "idf.sort_values()\n",
    "\n",
    "# Calculate TF-IDFs\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82db0c-4bf4-4cd2-9ddd-a9f4938048a2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Now you have preprocessed data to train your classification model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
