{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dlsun/pods/blob/master/10-Textual-Data/10.2%20The%20Vector%20Space%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYVDhxtNb74o"
   },
   "source": [
    "# 10.2 The Vector Space Model\n",
    "\n",
    "In the previous section, we saw how a single document could be converted into a _bag of words_ (or, more precisely, a bag of $n$-grams) representation. In this section, we go one step further, converting an entire corpus of documents into tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0WK4Y2qb74r"
   },
   "source": [
    "## Term Frequencies\n",
    "\n",
    "The bag of words representation gives us a mapping between words and their counts, such as `{..., \"am\": 3, \"i\": 71, \"sam\": 6, ...}`. To turn the bag of words into a vector of numbers, we can simply take the word counts, as follows:\n",
    "\n",
    "| ... | i  | am | sam | ... |\n",
    "|-----|----|----|-----|-----|\n",
    "| ... | 71 |  3 |  6  | ... |\n",
    "\n",
    "If we do this for each document in the corpus, and stack the rows, we obtain a table of numbers called the _term-frequency matrix_. \n",
    "\n",
    "|        | ... | i  | am | sam | ... |\n",
    "|--------|-----|----|----|-----|-----|\n",
    "|**green_eggs_and_ham**| ... | 71 |  3 |  6  | ... |\n",
    "|**cat_in_the_hat**| ... | 59 | 0 | 0 | ... |\n",
    "|**fox_in_socks**| ... | 13 | 0 | 0 | ... |\n",
    "|...|...|...|...|...|...|\n",
    "|**one_fish_two_fish**| ... | 51 | 3 | 0 | ... |\n",
    "\n",
    "The columns are all words (or _terms_) that appear in the corpus, which collectively make up the _vocabulary_. The idea of representing documents by a vector of numbers is called the _vector space model_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cgl6jLw3cErP"
   },
   "source": [
    "### Implementation from Scratch\n",
    "\n",
    "Let's obtain the term-frequency matrix for the Dr. Seuss books. First, we read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7Vy5COKb74s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "seuss_dir = \"http://dlsun.github.io/pods/data/drseuss/\"\n",
    "seuss_files = [\n",
    "    \"green_eggs_and_ham.txt\", \"cat_in_the_hat.txt\", \"fox_in_socks.txt\",\n",
    "    \"hop_on_pop.txt\", \"horton_hears_a_who.txt\", \"how_the_grinch_stole_christmas.txt\",\n",
    "    \"oh_the_places_youll_go.txt\", \"one_fish_two_fish.txt\"\n",
    "]\n",
    "\n",
    "docs_seuss = pd.Series()\n",
    "for file in seuss_files:\n",
    "    response = requests.get(seuss_dir + file, \"r\")\n",
    "    docs_seuss[file[:-4]] = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UOASR79b74x"
   },
   "source": [
    "Now we apply the bag of words representation to the normalized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6VrqvvJb74y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "green_eggs_and_ham                {'i': 84, 'am': 16, 'sam': 19, 'that': 3, 'do'...\n",
       "cat_in_the_hat                    {'the': 97, 'sun': 2, 'did': 10, 'not': 41, 's...\n",
       "fox_in_socks                      {'fox': 17, 'socks': 19, 'box': 7, 'knox': 17,...\n",
       "hop_on_pop                        {'up': 6, 'pup': 8, 'is': 12, 'cup': 4, 'in': ...\n",
       "horton_hears_a_who                {'on': 21, 'the': 97, 'fifteenth': 1, 'of': 39...\n",
       "how_the_grinch_stole_christmas    {'every': 5, 'who': 18, 'down': 10, 'in': 17, ...\n",
       "oh_the_places_youll_go            {'congratulations': 1, 'today': 2, 'is': 7, 'y...\n",
       "one_fish_two_fish                 {'one': 14, 'fish': 12, 'two': 4, 'red': 2, 'b...\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = (\n",
    "    docs_seuss.\n",
    "    str.lower().                  # convert all letters to lowercase\n",
    "    str.replace(\"[^\\w\\s]\", \" \").  # replace non-alphanumeric characters by whitespace\n",
    "    str.split()                   # split on whitespace\n",
    ").apply(Counter)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvgU4114b742"
   },
   "source": [
    "To turn this into a term-frequency matrix, we need to make a `DataFrame` out of it, where each column represents a word and each row a document---and each entry is the count of that word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylxYncocb744"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>am</th>\n",
       "      <th>sam</th>\n",
       "      <th>that</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>you</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>...</th>\n",
       "      <th>zeds</th>\n",
       "      <th>upon</th>\n",
       "      <th>heads</th>\n",
       "      <th>haircut</th>\n",
       "      <th>wave</th>\n",
       "      <th>swish</th>\n",
       "      <th>gack</th>\n",
       "      <th>park</th>\n",
       "      <th>clark</th>\n",
       "      <th>zeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>green_eggs_and_ham</th>\n",
       "      <td>84</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>36.0</td>\n",
       "      <td>82</td>\n",
       "      <td>44.0</td>\n",
       "      <td>34</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_in_the_hat</th>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>25.0</td>\n",
       "      <td>41</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox_in_socks</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hop_on_pop</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horton_hears_a_who</th>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how_the_grinch_stole_christmas</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh_the_places_youll_go</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one_fish_two_fish</th>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1355 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 i    am   sam  that    do  not  like  you  \\\n",
       "green_eggs_and_ham              84  16.0  19.0     3  36.0   82  44.0   34   \n",
       "cat_in_the_hat                  59   NaN   NaN    25  25.0   41  14.0   34   \n",
       "fox_in_socks                    13   NaN   NaN     6   8.0    1   1.0    8   \n",
       "hop_on_pop                       2   1.0   NaN     5   NaN    2   6.0    2   \n",
       "horton_hears_a_who              43   1.0   NaN    36   7.0    7   NaN   47   \n",
       "how_the_grinch_stole_christmas  16   NaN   NaN    16   4.0    2   2.0    2   \n",
       "oh_the_places_youll_go           6   NaN   NaN    12   4.0    9   1.0   85   \n",
       "one_fish_two_fish               51   3.0   NaN     1  12.0   10  21.0   24   \n",
       "\n",
       "                                green  eggs  ...  zeds  upon  heads  haircut  \\\n",
       "green_eggs_and_ham               10.0  10.0  ...   NaN   NaN    NaN      NaN   \n",
       "cat_in_the_hat                    NaN   NaN  ...   NaN   NaN    NaN      NaN   \n",
       "fox_in_socks                      NaN   NaN  ...   NaN   NaN    NaN      NaN   \n",
       "hop_on_pop                        NaN   NaN  ...   NaN   NaN    NaN      NaN   \n",
       "horton_hears_a_who                NaN   NaN  ...   NaN   NaN    NaN      NaN   \n",
       "how_the_grinch_stole_christmas    NaN   NaN  ...   NaN   NaN    NaN      NaN   \n",
       "oh_the_places_youll_go            NaN   NaN  ...   NaN   NaN    NaN      NaN   \n",
       "one_fish_two_fish                 NaN   NaN  ...   1.0   1.0    1.0      1.0   \n",
       "\n",
       "                                wave  swish  gack  park  clark  zeep  \n",
       "green_eggs_and_ham               NaN    NaN   NaN   NaN    NaN   NaN  \n",
       "cat_in_the_hat                   NaN    NaN   NaN   NaN    NaN   NaN  \n",
       "fox_in_socks                     NaN    NaN   NaN   NaN    NaN   NaN  \n",
       "hop_on_pop                       NaN    NaN   NaN   NaN    NaN   NaN  \n",
       "horton_hears_a_who               NaN    NaN   NaN   NaN    NaN   NaN  \n",
       "how_the_grinch_stole_christmas   NaN    NaN   NaN   NaN    NaN   NaN  \n",
       "oh_the_places_youll_go           NaN    NaN   NaN   NaN    NaN   NaN  \n",
       "one_fish_two_fish                1.0    3.0   2.0   1.0    1.0   1.0  \n",
       "\n",
       "[8 rows x 1355 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = pd.DataFrame(list(bag_of_words),index=bag_of_words.index)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836715867158671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(np.sum(tf.isnull()))/tf.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lAYV8ZPtb747"
   },
   "source": [
    "This matrix is full of missing numbers. A missing number means that the word did not appear in that document. In other words, a count of `NaN` really means a count of 0. So it makes sense in this situation to replace the `NaN`s by 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4p9mciwPb748"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>am</th>\n",
       "      <th>sam</th>\n",
       "      <th>that</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>you</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>...</th>\n",
       "      <th>zeds</th>\n",
       "      <th>upon</th>\n",
       "      <th>heads</th>\n",
       "      <th>haircut</th>\n",
       "      <th>wave</th>\n",
       "      <th>swish</th>\n",
       "      <th>gack</th>\n",
       "      <th>park</th>\n",
       "      <th>clark</th>\n",
       "      <th>zeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>green_eggs_and_ham</th>\n",
       "      <td>84</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>36.0</td>\n",
       "      <td>82</td>\n",
       "      <td>44.0</td>\n",
       "      <td>34</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_in_the_hat</th>\n",
       "      <td>59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25.0</td>\n",
       "      <td>41</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox_in_socks</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hop_on_pop</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horton_hears_a_who</th>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how_the_grinch_stole_christmas</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh_the_places_youll_go</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one_fish_two_fish</th>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1355 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 i    am   sam  that    do  not  like  you  \\\n",
       "green_eggs_and_ham              84  16.0  19.0     3  36.0   82  44.0   34   \n",
       "cat_in_the_hat                  59   0.0   0.0    25  25.0   41  14.0   34   \n",
       "fox_in_socks                    13   0.0   0.0     6   8.0    1   1.0    8   \n",
       "hop_on_pop                       2   1.0   0.0     5   0.0    2   6.0    2   \n",
       "horton_hears_a_who              43   1.0   0.0    36   7.0    7   0.0   47   \n",
       "how_the_grinch_stole_christmas  16   0.0   0.0    16   4.0    2   2.0    2   \n",
       "oh_the_places_youll_go           6   0.0   0.0    12   4.0    9   1.0   85   \n",
       "one_fish_two_fish               51   3.0   0.0     1  12.0   10  21.0   24   \n",
       "\n",
       "                                green  eggs  ...  zeds  upon  heads  haircut  \\\n",
       "green_eggs_and_ham               10.0  10.0  ...   0.0   0.0    0.0      0.0   \n",
       "cat_in_the_hat                    0.0   0.0  ...   0.0   0.0    0.0      0.0   \n",
       "fox_in_socks                      0.0   0.0  ...   0.0   0.0    0.0      0.0   \n",
       "hop_on_pop                        0.0   0.0  ...   0.0   0.0    0.0      0.0   \n",
       "horton_hears_a_who                0.0   0.0  ...   0.0   0.0    0.0      0.0   \n",
       "how_the_grinch_stole_christmas    0.0   0.0  ...   0.0   0.0    0.0      0.0   \n",
       "oh_the_places_youll_go            0.0   0.0  ...   0.0   0.0    0.0      0.0   \n",
       "one_fish_two_fish                 0.0   0.0  ...   1.0   1.0    1.0      1.0   \n",
       "\n",
       "                                wave  swish  gack  park  clark  zeep  \n",
       "green_eggs_and_ham               0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "cat_in_the_hat                   0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "fox_in_socks                     0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "hop_on_pop                       0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "horton_hears_a_who               0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "how_the_grinch_stole_christmas   0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "oh_the_places_youll_go           0.0    0.0   0.0   0.0    0.0   0.0  \n",
       "one_fish_two_fish                1.0    3.0   2.0   1.0    1.0   1.0  \n",
       "\n",
       "[8 rows x 1355 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tf.fillna(0)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(np.sum(tf.isnull()))/tf.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbTiSkqNb75B"
   },
   "source": [
    "### Implementation using `scikit-learn`\n",
    "\n",
    "We could have also used the `CountVectorizer` in `scikit-learn` to obtain the term-frequency matrix. This vectorizer is fit to a list of the documents in the corpus. By default, it converts all letters to lowercase and strips punctuation, although this behavior can be customized using the `strip_accents=` and `lowercase=` parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhl2Kwb5b75C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x1344 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2308 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "vec.fit(docs_seuss) # This determines the vocabulary.\n",
    "tf_sparse = vec.transform(docs_seuss)\n",
    "\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ni61EAub75G"
   },
   "source": [
    "Notice that `CountVectorizer` returns the term-frequency matrix, not as a `DataFrame` or even as a `numpy` array, but as a `scipy` sparse matrix. A _sparse matrix_ is one whose entries are mostly zeroes. For example,\n",
    "\n",
    "$$ \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 \\\\ 1.7 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -0.8 & 0 \\end{pmatrix} $$\n",
    "\n",
    "is an example of a sparse matrix. Instead of storing 20 values (most of which are equal to 0), we can simply store the locations of the non-zero entries and their values:\n",
    "\n",
    "- $(1, 0) \\rightarrow 1.7$\n",
    "- $(3, 3) \\rightarrow -0.8$\n",
    "\n",
    "All other entries of the matrix are assumed to be zero. This representation offers substantial memory savings when there are only a few non-zero entries. (But if not, then this representation can actually be more expensive.) Term-frequency matrices are usually sparse because most words do not appear in all documents.\n",
    "\n",
    "The `scipy` sparse matrix format is used to store sparse matrices. If necessary, a `scipy` sparse matrix can be converted to a `numpy` matrix using the `.todense()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61q9l5bbb75H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 3, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_sparse.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ibRYltDdb75L"
   },
   "source": [
    "We can further convert this `numpy` matrix to a `pandas` `DataFrame`. To make the column names descriptive, we call the `.get_feature_names()` method of the `CountVectorizer`, which returns a list of the words in the order that they appear in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKAtlcEDb75M"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12</th>\n",
       "      <th>56</th>\n",
       "      <th>98</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>act</th>\n",
       "      <th>afraid</th>\n",
       "      <th>after</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>again</th>\n",
       "      <th>...</th>\n",
       "      <th>yop</th>\n",
       "      <th>yopp</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yourselves</th>\n",
       "      <th>zans</th>\n",
       "      <th>zeds</th>\n",
       "      <th>zeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   12  56  98  able  about  act  afraid  after  afternoon  again  ...  yop  \\\n",
       "0   0   0   0     0      0    0       0      0          0      0  ...    0   \n",
       "1   0   0   0     0      3    0       0      1          0      0  ...    0   \n",
       "2   0   0   0     0      2    0       0      0          0      0  ...    0   \n",
       "3   0   0   0     0      0    0       0      2          0      0  ...    0   \n",
       "4   1   1   0     1      1    0       0      4          2      1  ...    0   \n",
       "5   0   0   0     0      0    0       0      0          0      0  ...    0   \n",
       "6   0   0   1     0      1    1       2      0          0      0  ...    0   \n",
       "7   0   0   0     0      1    0       0      0          0      2  ...    1   \n",
       "\n",
       "   yopp  you  young  your  yourself  yourselves  zans  zeds  zeep  \n",
       "0     0   34      0     0         0           0     0     0     0  \n",
       "1     0   34      0     8         0           0     0     0     0  \n",
       "2     0    8      0     1         0           0     0     0     0  \n",
       "3     0    2      0     0         0           0     0     0     0  \n",
       "4     3   47      5     7         0           1     0     0     0  \n",
       "5     0    2      1     0         0           0     0     0     0  \n",
       "6     0   85      0    20         2           0     0     0     0  \n",
       "7     0   24      0     9         0           0     3     1     1  \n",
       "\n",
       "[8 rows x 1344 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    tf_sparse.todense(),\n",
    "    columns=vec.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnompYDTb75P"
   },
   "source": [
    "The term-frequency matrix that `CountVectorizer` produced is not exactly the same as the matrix that we produced ourselves using just `pandas`. Although the two matrices have the same number of rows (8, corresponding to the number of documents in the corpus), they have a different number of columns. It appears that `CountVectorizer` had a vocabulary that was 11 words smaller (1344 words instead of 1355). We can determine exactly which 11 words these are, by taking the set difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IK31TXDvb75Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3', '4', '6', 'a', 'd', 'i', 'j', 'm', 'o', 's', 't'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tf.columns) - set(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tF1gsPqb75U"
   },
   "source": [
    "We see that all of the words that `CountVectorizer` missed were one-character long. By default, `CountVectorizer` only retains words that are at least 2 characters long. This behavior can be customized using the `token_pattern=` parameter, but we will not pursue that here, since 1-letter words are usually not useful for analysis anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5Pt4YzPb75V"
   },
   "source": [
    "`CountVectorizer` can even count $n$-grams. If we wanted both unigrams (i.e., individual words) and bigrams, then we would specify `ngram_range=(1, 2)`. If we wanted only the bigrams, then we would specify `ngram_range=(2, 2)`. \n",
    "\n",
    "Let's get unigrams, bigrams, and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xXPmH0rPb75W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x14918 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16560 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 3))\n",
    "vec.fit(docs_seuss)\n",
    "vec.transform(docs_seuss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRWacCYmb75Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16560"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of non-zero values in the sparse matrix.\n",
    "vec.transform(docs_seuss).count_nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbOmJw3Hb75c"
   },
   "source": [
    "There are nearly 15,000 bigrams. If we wanted to store this data in a `DataFrame`, we would need as many columns, even though only about 16,000 out of the nearly 120,000 entries are nonzero. This is why sparse matrices are vital in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvqfLNhNb75d"
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "The problem with term frequencies (TF) is that common words like \"the\" and \"that\" tend to have high counts and dominate. A better indicator of whether two documents are similar is if they share rare words. For example, the word \"eat\" only appears in two of the Dr. Seuss stories. The presence of that word in two documents is a strong indicator that the documents are similar, so we should give more weight to terms like it.\n",
    "\n",
    "This is the idea behind TF-IDF. We take each term frequency and re-weight it according to how many documents that term appears in (i.e., the **document frequency**). Since we want words that appear in fewer documents to get more weight, we take the **inverse document frequency** (IDF).  We take the logarithm of IDF because the distribution of IDFs is heavily skewed to the right. (Remember the discussion about transforming data from Chapter 1.4.) So in the end, the formula for IDF is:\n",
    "\n",
    "$$ \\textrm{idf}(t, D) = \\log \\frac{\\text{# of documents}}{\\text{# of documents containing $t$}} = \\log \\frac{|D|}{|d \\in D: t \\in d|}. $$\n",
    "\n",
    "(Sometimes, $1$ will be added to the denominator to prevent division by zero, if there are terms in the vocabulary that do not appear in the corpus.)\n",
    "\n",
    "To calculate TF-IDF, we simply multiply the term frequencies by the inverse document frequencies:\n",
    "\n",
    "$$ \\textrm{tf-idf}(d, t, D) = \\textrm{tf}(d, t) \\cdot \\textrm{idf}(t, D). $$\n",
    "\n",
    "Notice that unlike TF, the TF-IDF representation of a given document depends on the entire corpus of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7TZxq1jcP9A"
   },
   "source": [
    "### Implementation from Scratch\n",
    "\n",
    "Let's first see how to calculate TF-IDF from scratch, using the term-frequency matrix we obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6-b9Lgdb75e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i        8\n",
       "am       4\n",
       "sam      1\n",
       "that     8\n",
       "do       7\n",
       "        ..\n",
       "swish    1\n",
       "gack     1\n",
       "park     1\n",
       "clark    1\n",
       "zeep     1\n",
       "Length: 1355, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get document frequencies \n",
    "# (How many documents does each word appear in?)\n",
    "df = (tf > 0).sum(axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NP10ysjOb75i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i           0.000000\n",
       "a           0.000000\n",
       "in          0.000000\n",
       "they        0.000000\n",
       "be          0.000000\n",
       "              ...   \n",
       "days        2.079442\n",
       "gown        2.079442\n",
       "insisted    2.079442\n",
       "kites       2.079442\n",
       "zeep        2.079442\n",
       "Length: 1355, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get IDFs\n",
    "idf = np.log(len(tf) / df)\n",
    "idf.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H38ZejUGb75m"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>am</th>\n",
       "      <th>sam</th>\n",
       "      <th>that</th>\n",
       "      <th>do</th>\n",
       "      <th>not</th>\n",
       "      <th>like</th>\n",
       "      <th>you</th>\n",
       "      <th>green</th>\n",
       "      <th>eggs</th>\n",
       "      <th>...</th>\n",
       "      <th>zeds</th>\n",
       "      <th>upon</th>\n",
       "      <th>heads</th>\n",
       "      <th>haircut</th>\n",
       "      <th>wave</th>\n",
       "      <th>swish</th>\n",
       "      <th>gack</th>\n",
       "      <th>park</th>\n",
       "      <th>clark</th>\n",
       "      <th>zeep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>green_eggs_and_ham</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.090355</td>\n",
       "      <td>39.509389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.807130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.875381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.794415</td>\n",
       "      <td>20.794415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_in_the_hat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.338285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.869439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox_in_socks</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.068251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hop_on_pop</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.801188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horton_hears_a_who</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.934720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how_the_grinch_stole_christmas</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.267063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh_the_places_youll_go</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one_fish_two_fish</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.602377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.804159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>6.238325</td>\n",
       "      <td>4.158883</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1355 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  i         am        sam  that        do  \\\n",
       "green_eggs_and_ham              0.0  11.090355  39.509389   0.0  4.807130   \n",
       "cat_in_the_hat                  0.0   0.000000   0.000000   0.0  3.338285   \n",
       "fox_in_socks                    0.0   0.000000   0.000000   0.0  1.068251   \n",
       "hop_on_pop                      0.0   0.693147   0.000000   0.0  0.000000   \n",
       "horton_hears_a_who              0.0   0.693147   0.000000   0.0  0.934720   \n",
       "how_the_grinch_stole_christmas  0.0   0.000000   0.000000   0.0  0.534126   \n",
       "oh_the_places_youll_go          0.0   0.000000   0.000000   0.0  0.534126   \n",
       "one_fish_two_fish               0.0   2.079442   0.000000   0.0  1.602377   \n",
       "\n",
       "                                not      like  you      green       eggs  ...  \\\n",
       "green_eggs_and_ham              0.0  5.875381  0.0  20.794415  20.794415  ...   \n",
       "cat_in_the_hat                  0.0  1.869439  0.0   0.000000   0.000000  ...   \n",
       "fox_in_socks                    0.0  0.133531  0.0   0.000000   0.000000  ...   \n",
       "hop_on_pop                      0.0  0.801188  0.0   0.000000   0.000000  ...   \n",
       "horton_hears_a_who              0.0  0.000000  0.0   0.000000   0.000000  ...   \n",
       "how_the_grinch_stole_christmas  0.0  0.267063  0.0   0.000000   0.000000  ...   \n",
       "oh_the_places_youll_go          0.0  0.133531  0.0   0.000000   0.000000  ...   \n",
       "one_fish_two_fish               0.0  2.804159  0.0   0.000000   0.000000  ...   \n",
       "\n",
       "                                    zeds      upon     heads   haircut  \\\n",
       "green_eggs_and_ham              0.000000  0.000000  0.000000  0.000000   \n",
       "cat_in_the_hat                  0.000000  0.000000  0.000000  0.000000   \n",
       "fox_in_socks                    0.000000  0.000000  0.000000  0.000000   \n",
       "hop_on_pop                      0.000000  0.000000  0.000000  0.000000   \n",
       "horton_hears_a_who              0.000000  0.000000  0.000000  0.000000   \n",
       "how_the_grinch_stole_christmas  0.000000  0.000000  0.000000  0.000000   \n",
       "oh_the_places_youll_go          0.000000  0.000000  0.000000  0.000000   \n",
       "one_fish_two_fish               2.079442  2.079442  2.079442  2.079442   \n",
       "\n",
       "                                    wave     swish      gack      park  \\\n",
       "green_eggs_and_ham              0.000000  0.000000  0.000000  0.000000   \n",
       "cat_in_the_hat                  0.000000  0.000000  0.000000  0.000000   \n",
       "fox_in_socks                    0.000000  0.000000  0.000000  0.000000   \n",
       "hop_on_pop                      0.000000  0.000000  0.000000  0.000000   \n",
       "horton_hears_a_who              0.000000  0.000000  0.000000  0.000000   \n",
       "how_the_grinch_stole_christmas  0.000000  0.000000  0.000000  0.000000   \n",
       "oh_the_places_youll_go          0.000000  0.000000  0.000000  0.000000   \n",
       "one_fish_two_fish               2.079442  6.238325  4.158883  2.079442   \n",
       "\n",
       "                                   clark      zeep  \n",
       "green_eggs_and_ham              0.000000  0.000000  \n",
       "cat_in_the_hat                  0.000000  0.000000  \n",
       "fox_in_socks                    0.000000  0.000000  \n",
       "hop_on_pop                      0.000000  0.000000  \n",
       "horton_hears_a_who              0.000000  0.000000  \n",
       "how_the_grinch_stole_christmas  0.000000  0.000000  \n",
       "oh_the_places_youll_go          0.000000  0.000000  \n",
       "one_fish_two_fish               2.079442  2.079442  \n",
       "\n",
       "[8 rows x 1355 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF-IDFs\n",
    "tf_idf = tf * idf\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIhNpIHfb75q"
   },
   "source": [
    "### Implementation using `scikit-learn`\n",
    "\n",
    "We will not generally implement TF-IDF from scratch, like we did above. Instead, we will use Scikit-Learn's `TfidfVectorizer`, which operates similarly to `CountVectorizer`, except that it returns a matrix of the TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fbu5oi7ib75r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x1344 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2308 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(norm=None) # Do not normalize.\n",
    "vec.fit(docs_seuss) # This determines the vocabulary.\n",
    "tf_idf_sparse = vec.transform(docs_seuss)\n",
    "tf_idf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cycHn7Cjb75v"
   },
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "We now have a representation of each text document as a vector of numbers. Each number can either be a term frequency or a TF-IDF weight. We can visualize each vector as an arrow in a high-dimensional space, where each dimension represents a word. The magnitude of the vector along a dimension represents the \"frequency\" (TF or TF-IDF) of that word in the document. For example, if our vocabulary only contains two words, \"i\" and \"sam\", then the arrows shown below might represent two documents:\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "To fit $k$-nearest neighbors or $k$-means clustering, we need some way to measure the distance between two documents (i.e., two vectors). We could use Euclidean distance, as we have done in the past.\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_euclidean.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "But Euclidean distance does not make sense for TF or TF-IDF vectors. To see why, consider the two documents:\n",
    "\n",
    "1. \"I am Sam.\" \n",
    "2. \"I am Sam. Sam I am.\" \n",
    "\n",
    "The two documents are obviously very similar. But the vector for the second is twice as long as the vector for the first because the second document has twice as many occurrences of each word. This is true whether we use TF or TF-IDF weights. If we calculate the Euclidean distance between these two vectors, then they will seem quite far apart.\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_example.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "With TF and TF-IDF vectors, the distinguishing property is their _direction_. Because the two vectors above point in the same direction, they are similar. We need a distance metric that measures how different their directions are. A natural way to measure the difference between the directions of two vectors is the angle between them.\n",
    "\n",
    "<img src=\"https://github.com/dlsun/pods/blob/master/10-Textual-Data/vector_space_cosine.png?raw=1\" width=\"300\"/>\n",
    "\n",
    "The cosine of the angle between two vectors ${\\bf a}$ and ${\\bf b}$ can be calculated as:\n",
    "\n",
    "$$ \\cos \\theta = \\frac{\\sum a_j b_j}{\\sqrt{\\sum a_j^2} \\sqrt{\\sum b_j^2}}. $$\n",
    "\n",
    "Although it is possible to work out the angle $\\theta$ from this formula, it is more common to report $\\cos\\theta$ as a measure of similarity between two vectors. This similarity metric is called **cosine similarity**. Notice that when the angle $\\theta$ is close to 0 (i.e., when the two vectors point in nearly the same direction), the value of $\\cos\\theta$ is high (close to 1.0, which is the maximum possible value).\n",
    "\n",
    "The cosine _distance_ is defined as 1 minus the similarity. This makes it so that 0 means that the two vectors point in the same direction:\n",
    "\n",
    "$$ d_{\\cos}({\\bf a}, {\\bf b}) = 1 - \\cos(\\theta({\\bf a}, {\\bf b})) = 1 - \\frac{\\sum a_j b_j}{\\sqrt{\\sum a_j^2} \\sqrt{\\sum b_j^2}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rQbvOkVcbvK"
   },
   "source": [
    "### Implementation from Scratch\n",
    "\n",
    "Let's calculate the cosine similarity between document 0 (_Green Eggs and Ham_) and document 2 (_Fox in Socks_) using the TF-IDF representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kagfHiKb75w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10197809112431884"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the numerator.\n",
    "a = tf_idf_sparse[0, :]\n",
    "b = tf_idf_sparse[2, :]\n",
    "dot = a.multiply(b).sum()\n",
    "\n",
    "# Calculate the terms in the denominator.\n",
    "a_len = np.sqrt(a.multiply(a).sum())\n",
    "b_len = np.sqrt(b.multiply(b).sum())\n",
    "\n",
    "# Cosine similarity is their ratio.\n",
    "dot / (a_len * b_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bqj82-h2b750"
   },
   "source": [
    "These two vectors are not very similar, as evidenced by their low cosine similarity (close to 0.0). Let's try to find the most similar documents in the corpus to _Green Eggs and Ham_---in other words, its nearest neighbors. To do this, we will take advantage of _broadcasting_: we will multiply a TF-IDF vector (representing document 0) by the entire TF-IDF matrix and calculate the sum over the columns. This will give us a vector of dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Nr27ArFb751"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[34282.66548742],\n",
       "        [13856.74242973],\n",
       "        [ 3192.73842529],\n",
       "        [ 1662.65737991],\n",
       "        [ 8698.41557824],\n",
       "        [ 5098.5714281 ],\n",
       "        [ 6918.05569539],\n",
       "        [ 6958.3624518 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the numerators.\n",
    "a = tf_idf_sparse[0, :]\n",
    "B = tf_idf_sparse\n",
    "dot = a.multiply(B).sum(axis=1)\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tuXVdFAb754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.15578707514322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[185.15578708],\n",
       "        [220.67601821],\n",
       "        [169.09048515],\n",
       "        [ 67.77450402],\n",
       "        [246.13363988],\n",
       "        [208.51785767],\n",
       "        [151.24831788],\n",
       "        [151.8499661 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the denominators.\n",
    "a_len = np.sqrt(a.multiply(a).sum())\n",
    "b_len = np.sqrt(B.multiply(B).sum(axis=1))\n",
    "print(a_len)\n",
    "b_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m72blwPNb76A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.        ],\n",
       "        [0.33913196],\n",
       "        [0.10197809],\n",
       "        [0.13249489],\n",
       "        [0.19086746],\n",
       "        [0.13205899],\n",
       "        [0.2470337 ],\n",
       "        [0.24748852]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate their ratio to obtain cosine similarities.\n",
    "dot / (a_len * b_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEuizyNvb76E"
   },
   "source": [
    "Now let's put this matrix into a `DataFrame` so that we can easily sort the values in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IWk1cYEb76E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.000000\n",
       "1    0.339132\n",
       "7    0.247489\n",
       "6    0.247034\n",
       "4    0.190867\n",
       "3    0.132495\n",
       "5    0.132059\n",
       "2    0.101978\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities = pd.DataFrame(dot / (a_len * b_len))[0]\n",
    "most_similar = cos_similarities.sort_values(ascending=False)\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ELG94ygFb76H"
   },
   "source": [
    "Of course, the most similar document in the corpus to _Green Eggs and Ham_ (with a perfect cosine similarity of 1.0) is itself. But the next most similar text is _The Cat in the Hat_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBVAM4hmb76I"
   },
   "source": [
    "### Implementation using scikit-learn\n",
    "\n",
    "It is also possible to calculate cosine similarities/distances in `scikit-learn` using the same API that we used to calculate distances in Chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZoMACMMb76J"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>green_eggs_and_ham</th>\n",
       "      <th>cat_in_the_hat</th>\n",
       "      <th>fox_in_socks</th>\n",
       "      <th>hop_on_pop</th>\n",
       "      <th>horton_hears_a_who</th>\n",
       "      <th>how_the_grinch_stole_christmas</th>\n",
       "      <th>oh_the_places_youll_go</th>\n",
       "      <th>one_fish_two_fish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>green_eggs_and_ham</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339132</td>\n",
       "      <td>0.101978</td>\n",
       "      <td>0.132495</td>\n",
       "      <td>0.190867</td>\n",
       "      <td>0.132059</td>\n",
       "      <td>0.247034</td>\n",
       "      <td>0.247489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat_in_the_hat</th>\n",
       "      <td>0.339132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190015</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.588969</td>\n",
       "      <td>0.539433</td>\n",
       "      <td>0.442517</td>\n",
       "      <td>0.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox_in_socks</th>\n",
       "      <td>0.101978</td>\n",
       "      <td>0.190015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.097093</td>\n",
       "      <td>0.171208</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>0.154492</td>\n",
       "      <td>0.193159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hop_on_pop</th>\n",
       "      <td>0.132495</td>\n",
       "      <td>0.332097</td>\n",
       "      <td>0.097093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.254134</td>\n",
       "      <td>0.214191</td>\n",
       "      <td>0.147176</td>\n",
       "      <td>0.368720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horton_hears_a_who</th>\n",
       "      <td>0.190867</td>\n",
       "      <td>0.588969</td>\n",
       "      <td>0.171208</td>\n",
       "      <td>0.254134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.594556</td>\n",
       "      <td>0.485144</td>\n",
       "      <td>0.480848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how_the_grinch_stole_christmas</th>\n",
       "      <td>0.132059</td>\n",
       "      <td>0.539433</td>\n",
       "      <td>0.128882</td>\n",
       "      <td>0.214191</td>\n",
       "      <td>0.594556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300953</td>\n",
       "      <td>0.377783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oh_the_places_youll_go</th>\n",
       "      <td>0.247034</td>\n",
       "      <td>0.442517</td>\n",
       "      <td>0.154492</td>\n",
       "      <td>0.147176</td>\n",
       "      <td>0.485144</td>\n",
       "      <td>0.300953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.404552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one_fish_two_fish</th>\n",
       "      <td>0.247489</td>\n",
       "      <td>0.613000</td>\n",
       "      <td>0.193159</td>\n",
       "      <td>0.368720</td>\n",
       "      <td>0.480848</td>\n",
       "      <td>0.377783</td>\n",
       "      <td>0.404552</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                green_eggs_and_ham  cat_in_the_hat  \\\n",
       "green_eggs_and_ham                        1.000000        0.339132   \n",
       "cat_in_the_hat                            0.339132        1.000000   \n",
       "fox_in_socks                              0.101978        0.190015   \n",
       "hop_on_pop                                0.132495        0.332097   \n",
       "horton_hears_a_who                        0.190867        0.588969   \n",
       "how_the_grinch_stole_christmas            0.132059        0.539433   \n",
       "oh_the_places_youll_go                    0.247034        0.442517   \n",
       "one_fish_two_fish                         0.247489        0.613000   \n",
       "\n",
       "                                fox_in_socks  hop_on_pop  horton_hears_a_who  \\\n",
       "green_eggs_and_ham                  0.101978    0.132495            0.190867   \n",
       "cat_in_the_hat                      0.190015    0.332097            0.588969   \n",
       "fox_in_socks                        1.000000    0.097093            0.171208   \n",
       "hop_on_pop                          0.097093    1.000000            0.254134   \n",
       "horton_hears_a_who                  0.171208    0.254134            1.000000   \n",
       "how_the_grinch_stole_christmas      0.128882    0.214191            0.594556   \n",
       "oh_the_places_youll_go              0.154492    0.147176            0.485144   \n",
       "one_fish_two_fish                   0.193159    0.368720            0.480848   \n",
       "\n",
       "                                how_the_grinch_stole_christmas  \\\n",
       "green_eggs_and_ham                                    0.132059   \n",
       "cat_in_the_hat                                        0.539433   \n",
       "fox_in_socks                                          0.128882   \n",
       "hop_on_pop                                            0.214191   \n",
       "horton_hears_a_who                                    0.594556   \n",
       "how_the_grinch_stole_christmas                        1.000000   \n",
       "oh_the_places_youll_go                                0.300953   \n",
       "one_fish_two_fish                                     0.377783   \n",
       "\n",
       "                                oh_the_places_youll_go  one_fish_two_fish  \n",
       "green_eggs_and_ham                            0.247034           0.247489  \n",
       "cat_in_the_hat                                0.442517           0.613000  \n",
       "fox_in_socks                                  0.154492           0.193159  \n",
       "hop_on_pop                                    0.147176           0.368720  \n",
       "horton_hears_a_who                            0.485144           0.480848  \n",
       "how_the_grinch_stole_christmas                0.300953           0.377783  \n",
       "oh_the_places_youll_go                        1.000000           0.404552  \n",
       "one_fish_two_fish                             0.404552           1.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "CSM = pd.DataFrame(cosine_similarity(tf_idf_sparse),index=docs_seuss.index,columns=docs_seuss.index)\n",
    "CSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fox_in_socks                    hop_on_pop                        0.097093\n",
       "green_eggs_and_ham              fox_in_socks                      0.101978\n",
       "fox_in_socks                    how_the_grinch_stole_christmas    0.128882\n",
       "green_eggs_and_ham              how_the_grinch_stole_christmas    0.132059\n",
       "                                hop_on_pop                        0.132495\n",
       "hop_on_pop                      oh_the_places_youll_go            0.147176\n",
       "fox_in_socks                    oh_the_places_youll_go            0.154492\n",
       "                                horton_hears_a_who                0.171208\n",
       "cat_in_the_hat                  fox_in_socks                      0.190015\n",
       "green_eggs_and_ham              horton_hears_a_who                0.190867\n",
       "fox_in_socks                    one_fish_two_fish                 0.193159\n",
       "hop_on_pop                      how_the_grinch_stole_christmas    0.214191\n",
       "green_eggs_and_ham              oh_the_places_youll_go            0.247034\n",
       "                                one_fish_two_fish                 0.247489\n",
       "hop_on_pop                      horton_hears_a_who                0.254134\n",
       "how_the_grinch_stole_christmas  oh_the_places_youll_go            0.300953\n",
       "cat_in_the_hat                  hop_on_pop                        0.332097\n",
       "green_eggs_and_ham              cat_in_the_hat                    0.339132\n",
       "hop_on_pop                      one_fish_two_fish                 0.368720\n",
       "how_the_grinch_stole_christmas  one_fish_two_fish                 0.377783\n",
       "oh_the_places_youll_go          one_fish_two_fish                 0.404552\n",
       "cat_in_the_hat                  oh_the_places_youll_go            0.442517\n",
       "horton_hears_a_who              one_fish_two_fish                 0.480848\n",
       "                                oh_the_places_youll_go            0.485144\n",
       "cat_in_the_hat                  how_the_grinch_stole_christmas    0.539433\n",
       "                                horton_hears_a_who                0.588969\n",
       "horton_hears_a_who              how_the_grinch_stole_christmas    0.594556\n",
       "cat_in_the_hat                  one_fish_two_fish                 0.613000\n",
       "oh_the_places_youll_go          oh_the_places_youll_go            1.000000\n",
       "fox_in_socks                    fox_in_socks                      1.000000\n",
       "cat_in_the_hat                  cat_in_the_hat                    1.000000\n",
       "horton_hears_a_who              horton_hears_a_who                1.000000\n",
       "green_eggs_and_ham              green_eggs_and_ham                1.000000\n",
       "hop_on_pop                      hop_on_pop                        1.000000\n",
       "how_the_grinch_stole_christmas  how_the_grinch_stole_christmas    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSM.stack().drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTWQae5xb76M"
   },
   "source": [
    "The $(i, j)$th entry of this matrix represents the cosine similarity between the $i$th and $j$th documents. So the first row of this matrix contains the similarities between _Green Eggs and Ham_ and the other documents in the corpus. Check that these numbers match the ones we obtained manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vm7f6Jncb76N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.33913196, 0.10197809, 0.13249489, 0.19086746,\n",
       "       0.13205899, 0.2470337 , 0.24748852])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf_sparse)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMu61Qx5b76Q"
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBojZWgHb76R"
   },
   "source": [
    "1\\. Suppose we had instead compared documents using cosine similarity on the term frequencies (TF), instead of TF-IDF. Does this change the conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "green_eggs_and_ham              how_the_grinch_stole_christmas    0.196255\n",
       "                                hop_on_pop                        0.209853\n",
       "                                fox_in_socks                      0.219427\n",
       "fox_in_socks                    hop_on_pop                        0.253080\n",
       "hop_on_pop                      oh_the_places_youll_go            0.253759\n",
       "green_eggs_and_ham              horton_hears_a_who                0.286314\n",
       "fox_in_socks                    how_the_grinch_stole_christmas    0.293803\n",
       "                                oh_the_places_youll_go            0.315316\n",
       "hop_on_pop                      how_the_grinch_stole_christmas    0.321074\n",
       "green_eggs_and_ham              one_fish_two_fish                 0.344958\n",
       "                                oh_the_places_youll_go            0.353771\n",
       "fox_in_socks                    one_fish_two_fish                 0.374255\n",
       "                                horton_hears_a_who                0.377986\n",
       "cat_in_the_hat                  fox_in_socks                      0.384181\n",
       "hop_on_pop                      horton_hears_a_who                0.396915\n",
       "how_the_grinch_stole_christmas  oh_the_places_youll_go            0.406152\n",
       "cat_in_the_hat                  hop_on_pop                        0.453130\n",
       "green_eggs_and_ham              cat_in_the_hat                    0.456355\n",
       "how_the_grinch_stole_christmas  one_fish_two_fish                 0.482370\n",
       "hop_on_pop                      one_fish_two_fish                 0.521146\n",
       "oh_the_places_youll_go          one_fish_two_fish                 0.531074\n",
       "cat_in_the_hat                  oh_the_places_youll_go            0.570050\n",
       "horton_hears_a_who              one_fish_two_fish                 0.631293\n",
       "                                oh_the_places_youll_go            0.643735\n",
       "cat_in_the_hat                  one_fish_two_fish                 0.695295\n",
       "                                how_the_grinch_stole_christmas    0.718001\n",
       "                                horton_hears_a_who                0.777937\n",
       "horton_hears_a_who              how_the_grinch_stole_christmas    0.790066\n",
       "cat_in_the_hat                  cat_in_the_hat                    1.000000\n",
       "fox_in_socks                    fox_in_socks                      1.000000\n",
       "green_eggs_and_ham              green_eggs_and_ham                1.000000\n",
       "oh_the_places_youll_go          oh_the_places_youll_go            1.000000\n",
       "one_fish_two_fish               one_fish_two_fish                 1.000000\n",
       "hop_on_pop                      hop_on_pop                        1.000000\n",
       "how_the_grinch_stole_christmas  how_the_grinch_stole_christmas    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "CSM = pd.DataFrame(cosine_similarity(tf_sparse),index=docs_seuss.index,columns=docs_seuss.index)\n",
    "CSM.stack().drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEupF68-b76R"
   },
   "source": [
    "2\\. Suppose we had instead used Euclidean distance on the TF-IDF weights, instead of cosine distance. Does this change the conclusion? What if we first normalize the length of the TF-IDF vector for each document before calculating Euclidean distance?\n",
    "\n",
    "_Challenge Exercise:_ Can you prove the above fact mathematically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "green_eggs_and_ham              horton_hears_a_who                278.330025\n",
       "fox_in_socks                    horton_hears_a_who                273.719560\n",
       "green_eggs_and_ham              how_the_grinch_stole_christmas    259.933106\n",
       "how_the_grinch_stole_christmas  green_eggs_and_ham                259.933106\n",
       "cat_in_the_hat                  fox_in_socks                      251.214995\n",
       "how_the_grinch_stole_christmas  fox_in_socks                      250.964003\n",
       "fox_in_socks                    how_the_grinch_stole_christmas    250.964003\n",
       "hop_on_pop                      horton_hears_a_who                238.110143\n",
       "green_eggs_and_ham              fox_in_socks                      237.673686\n",
       "                                cat_in_the_hat                    235.089527\n",
       "dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, cosine_distances\n",
    "\n",
    "ESM1 = pd.DataFrame(euclidean_distances(tf_idf_sparse),index=docs_seuss.index,columns=docs_seuss.index)\n",
    "ESM1.stack().drop_duplicates().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fox_in_socks        hop_on_pop                        0.097093\n",
       "green_eggs_and_ham  fox_in_socks                      0.101978\n",
       "fox_in_socks        how_the_grinch_stole_christmas    0.128882\n",
       "green_eggs_and_ham  how_the_grinch_stole_christmas    0.132059\n",
       "                    hop_on_pop                        0.132495\n",
       "hop_on_pop          oh_the_places_youll_go            0.147176\n",
       "fox_in_socks        oh_the_places_youll_go            0.154492\n",
       "                    horton_hears_a_who                0.171208\n",
       "cat_in_the_hat      fox_in_socks                      0.190015\n",
       "green_eggs_and_ham  horton_hears_a_who                0.190867\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "\n",
    "CSM2 = pd.DataFrame(cosine_similarity(tf_idf_sparse),index=docs_seuss.index,columns=docs_seuss.index)\n",
    "CSM2.stack().drop_duplicates().sort_values().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_in_the_hat          oh_the_places_youll_go            205.022590\n",
       "hop_on_pop              how_the_grinch_stole_christmas    204.985588\n",
       "fox_in_socks            one_fish_two_fish                 204.281037\n",
       "green_eggs_and_ham      hop_on_pop                        188.549023\n",
       "fox_in_socks            hop_on_pop                        175.953381\n",
       "cat_in_the_hat          one_fish_two_fish                 175.138540\n",
       "oh_the_places_youll_go  one_fish_two_fish                 165.383568\n",
       "hop_on_pop              oh_the_places_youll_go            156.371659\n",
       "                        one_fish_two_fish                 141.641796\n",
       "green_eggs_and_ham      green_eggs_and_ham                  0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESM1.stack().drop_duplicates().sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "green_eggs_and_ham  horton_hears_a_who                0.190867\n",
       "cat_in_the_hat      fox_in_socks                      0.190015\n",
       "fox_in_socks        horton_hears_a_who                0.171208\n",
       "                    oh_the_places_youll_go            0.154492\n",
       "hop_on_pop          oh_the_places_youll_go            0.147176\n",
       "green_eggs_and_ham  hop_on_pop                        0.132495\n",
       "                    how_the_grinch_stole_christmas    0.132059\n",
       "fox_in_socks        how_the_grinch_stole_christmas    0.128882\n",
       "green_eggs_and_ham  fox_in_socks                      0.101978\n",
       "fox_in_socks        hop_on_pop                        0.097093\n",
       "dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSM2.stack().drop_duplicates().sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1334</th>\n",
       "      <th>1335</th>\n",
       "      <th>1336</th>\n",
       "      <th>1337</th>\n",
       "      <th>1338</th>\n",
       "      <th>1339</th>\n",
       "      <th>1340</th>\n",
       "      <th>1341</th>\n",
       "      <th>1342</th>\n",
       "      <th>1343</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010174</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029430</td>\n",
       "      <td>0.020347</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.030521</td>\n",
       "      <td>0.190953</td>\n",
       "      <td>0.042632</td>\n",
       "      <td>0.039971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009592</td>\n",
       "      <td>0.010064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.016556</td>\n",
       "      <td>0.033112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185849</td>\n",
       "      <td>0.033112</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049471</td>\n",
       "      <td>0.01649</td>\n",
       "      <td>0.01649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1344 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2         3         4         5         6     \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.019107  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.016624  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.010174  0.010174  0.000000  0.010174  0.005710  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000  0.000000  0.016556  0.000000  0.009292  0.016556  0.033112   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.009256  0.000000  0.000000   \n",
       "\n",
       "       7         8         9     ...     1334      1335      1336      1337  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.183629  0.000000   \n",
       "1  0.008206  0.000000  0.000000  ...  0.00000  0.000000  0.154072  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.047312  0.000000   \n",
       "3  0.053440  0.000000  0.000000  ...  0.00000  0.000000  0.029510  0.000000   \n",
       "4  0.029430  0.020347  0.008526  ...  0.00000  0.030521  0.190953  0.042632   \n",
       "5  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.009592  0.010064   \n",
       "6  0.000000  0.000000  0.000000  ...  0.00000  0.000000  0.561990  0.000000   \n",
       "7  0.000000  0.000000  0.027641  ...  0.01649  0.000000  0.158051  0.000000   \n",
       "\n",
       "       1338      1339      1340      1341     1342     1343  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  \n",
       "1  0.050951  0.000000  0.000000  0.000000  0.00000  0.00000  \n",
       "2  0.008312  0.000000  0.000000  0.000000  0.00000  0.00000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  \n",
       "4  0.039971  0.000000  0.010174  0.000000  0.00000  0.00000  \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.00000  0.00000  \n",
       "6  0.185849  0.033112  0.000000  0.000000  0.00000  0.00000  \n",
       "7  0.083301  0.000000  0.000000  0.049471  0.01649  0.01649  \n",
       "\n",
       "[8 rows x 1344 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf = pd.DataFrame.sparse.from_spmatrix(tf_idf_sparse)\n",
    "df_tf_idf_norm = df_tf_idf.divide(np.sqrt((df_tf_idf**2).sum(axis=1)),axis=0)\n",
    "df_tf_idf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "5    1.0\n",
       "6    1.0\n",
       "7    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((df_tf_idf_norm**2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fox_in_socks        hop_on_pop                        1.343806\n",
       "green_eggs_and_ham  fox_in_socks                      1.340166\n",
       "fox_in_socks        how_the_grinch_stole_christmas    1.319938\n",
       "green_eggs_and_ham  how_the_grinch_stole_christmas    1.317529\n",
       "                    hop_on_pop                        1.317198\n",
       "hop_on_pop          oh_the_places_youll_go            1.306004\n",
       "fox_in_socks        oh_the_places_youll_go            1.300391\n",
       "                    horton_hears_a_who                1.287472\n",
       "cat_in_the_hat      fox_in_socks                      1.272780\n",
       "green_eggs_and_ham  horton_hears_a_who                1.272110\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, cosine_distances\n",
    "\n",
    "ESM2 = pd.DataFrame(euclidean_distances(df_tf_idf_norm),index=docs_seuss.index,columns=docs_seuss.index)\n",
    "ESM2.stack().drop_duplicates().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fox_in_socks        hop_on_pop                        0.097093\n",
       "green_eggs_and_ham  fox_in_socks                      0.101978\n",
       "fox_in_socks        how_the_grinch_stole_christmas    0.128882\n",
       "green_eggs_and_ham  how_the_grinch_stole_christmas    0.132059\n",
       "                    hop_on_pop                        0.132495\n",
       "hop_on_pop          oh_the_places_youll_go            0.147176\n",
       "fox_in_socks        oh_the_places_youll_go            0.154492\n",
       "                    horton_hears_a_who                0.171208\n",
       "cat_in_the_hat      fox_in_socks                      0.190015\n",
       "green_eggs_and_ham  horton_hears_a_who                0.190867\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSM2.stack().drop_duplicates().sort_values().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "green_eggs_and_ham              horton_hears_a_who                278.330025\n",
       "fox_in_socks                    horton_hears_a_who                273.719560\n",
       "green_eggs_and_ham              how_the_grinch_stole_christmas    259.933106\n",
       "how_the_grinch_stole_christmas  green_eggs_and_ham                259.933106\n",
       "cat_in_the_hat                  fox_in_socks                      251.214995\n",
       "how_the_grinch_stole_christmas  fox_in_socks                      250.964003\n",
       "fox_in_socks                    how_the_grinch_stole_christmas    250.964003\n",
       "hop_on_pop                      horton_hears_a_who                238.110143\n",
       "green_eggs_and_ham              fox_in_socks                      237.673686\n",
       "                                cat_in_the_hat                    235.089527\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESM1.stack().drop_duplicates().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_jiS5xzpWx2"
   },
   "source": [
    "3\\. Convert the self-summary variable (`essay0`) in the OKCupid data set (https://dlsun.github.io/pods/data/okcupid.csv) to a TF-IDF representation. Use this to find a match for user 61 based on what he says he is looking for in a partner (`essay9`).\n",
    "\n",
    "The [data dictionary](https://github.com/rudeboybert/JSE_OkCupid/blob/master/okcupid_codebook.txt) may help you understand what the columns mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>...</th>\n",
       "      <th>location</th>\n",
       "      <th>offspring</th>\n",
       "      <th>orientation</th>\n",
       "      <th>pets</th>\n",
       "      <th>religion</th>\n",
       "      <th>sex</th>\n",
       "      <th>sign</th>\n",
       "      <th>smokes</th>\n",
       "      <th>height</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mostly vegetarian</td>\n",
       "      <td>socially</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>75% nice, 45% shy, 80% stubborn, 100% charming...</td>\n",
       "      <td>i'm a new nurse. it rules.</td>\n",
       "      <td>multiple-choice questions, dancing.</td>\n",
       "      <td>it depends on the people.</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>might want kids</td>\n",
       "      <td>gay</td>\n",
       "      <td>likes cats</td>\n",
       "      <td>buddhism</td>\n",
       "      <td>f</td>\n",
       "      <td>taurus and it&amp;rsquo;s fun to think about</td>\n",
       "      <td>no</td>\n",
       "      <td>67.0</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>average</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>i like trees, spending long periods of time co...</td>\n",
       "      <td>studying landscape horticulture, beekeeping, g...</td>\n",
       "      <td>wasting time, making breakfast, nesting</td>\n",
       "      <td>i have a lot of freckles</td>\n",
       "      <td>...</td>\n",
       "      <td>oakland, california</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>sagittarius and it&amp;rsquo;s fun to think about</td>\n",
       "      <td>no</td>\n",
       "      <td>66.0</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>curvy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rarely</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>has a kid</td>\n",
       "      <td>straight</td>\n",
       "      <td>likes dogs and has cats</td>\n",
       "      <td>other and laughing about it</td>\n",
       "      <td>f</td>\n",
       "      <td>leo and it&amp;rsquo;s fun to think about</td>\n",
       "      <td>trying to quit</td>\n",
       "      <td>65.0</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>average</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am a seeker of laughs ,music ,magick good pe...</td>\n",
       "      <td>i strive to live life to the fullest and to tr...</td>\n",
       "      <td>i am good at my magic and weaving a world of i...</td>\n",
       "      <td>i am guessing y'all would notice my jewelry an...</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>doesn&amp;rsquo;t want kids</td>\n",
       "      <td>gay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other and very serious about it</td>\n",
       "      <td>m</td>\n",
       "      <td>capricorn and it&amp;rsquo;s fun to think about</td>\n",
       "      <td>trying to quit</td>\n",
       "      <td>70.0</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from ph.d program</td>\n",
       "      <td>i've just moved here from london after finishi...</td>\n",
       "      <td>i'm doing a postdoc in psychology at stanford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m</td>\n",
       "      <td>cancer but it doesn&amp;rsquo;t matter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.0</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age body_type               diet    drinks      drugs  \\\n",
       "0   31       NaN  mostly vegetarian  socially  sometimes   \n",
       "1   25   average                NaN  socially        NaN   \n",
       "2   43     curvy                NaN    rarely      never   \n",
       "3   31   average                NaN  socially      never   \n",
       "4   34       NaN                NaN  socially        NaN   \n",
       "\n",
       "                           education  \\\n",
       "0  graduated from college/university   \n",
       "1      working on college/university   \n",
       "2     graduated from masters program   \n",
       "3                                NaN   \n",
       "4        graduated from ph.d program   \n",
       "\n",
       "                                              essay0  \\\n",
       "0  75% nice, 45% shy, 80% stubborn, 100% charming...   \n",
       "1  i like trees, spending long periods of time co...   \n",
       "2                                                NaN   \n",
       "3  i am a seeker of laughs ,music ,magick good pe...   \n",
       "4  i've just moved here from london after finishi...   \n",
       "\n",
       "                                              essay1  \\\n",
       "0                         i'm a new nurse. it rules.   \n",
       "1  studying landscape horticulture, beekeeping, g...   \n",
       "2                                                NaN   \n",
       "3  i strive to live life to the fullest and to tr...   \n",
       "4      i'm doing a postdoc in psychology at stanford   \n",
       "\n",
       "                                              essay2  \\\n",
       "0                multiple-choice questions, dancing.   \n",
       "1            wasting time, making breakfast, nesting   \n",
       "2                                                NaN   \n",
       "3  i am good at my magic and weaving a world of i...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              essay3  ...  \\\n",
       "0                          it depends on the people.  ...   \n",
       "1                           i have a lot of freckles  ...   \n",
       "2                                                NaN  ...   \n",
       "3  i am guessing y'all would notice my jewelry an...  ...   \n",
       "4                                                NaN  ...   \n",
       "\n",
       "                    location                offspring orientation  \\\n",
       "0  san francisco, california          might want kids         gay   \n",
       "1        oakland, california                      NaN         gay   \n",
       "2  san francisco, california                has a kid    straight   \n",
       "3  san francisco, california  doesn&rsquo;t want kids         gay   \n",
       "4  san francisco, california                      NaN         gay   \n",
       "\n",
       "                      pets                         religion sex  \\\n",
       "0               likes cats                         buddhism   f   \n",
       "1                      NaN                              NaN   m   \n",
       "2  likes dogs and has cats      other and laughing about it   f   \n",
       "3                      NaN  other and very serious about it   m   \n",
       "4                      NaN                              NaN   m   \n",
       "\n",
       "                                            sign          smokes  height  \\\n",
       "0       taurus and it&rsquo;s fun to think about              no    67.0   \n",
       "1  sagittarius and it&rsquo;s fun to think about              no    66.0   \n",
       "2          leo and it&rsquo;s fun to think about  trying to quit    65.0   \n",
       "3    capricorn and it&rsquo;s fun to think about  trying to quit    70.0   \n",
       "4             cancer but it doesn&rsquo;t matter             NaN    71.0   \n",
       "\n",
       "   status  \n",
       "0  single  \n",
       "1  single  \n",
       "2  single  \n",
       "3  single  \n",
       "4  single  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_okcupid = pd.read_csv('https://dlsun.github.io/pods/data/okcupid.csv')\n",
    "df_okcupid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       75% nice, 45% shy, 80% stubborn, 100% charming...\n",
       "1       i like trees, spending long periods of time co...\n",
       "2                                                        \n",
       "3       i am a seeker of laughs ,music ,magick good pe...\n",
       "4       i've just moved here from london after finishi...\n",
       "                              ...                        \n",
       "2995              you can woo a man with your vocabulary.\n",
       "2996                                                     \n",
       "2997                                                     \n",
       "2998    you've got the know-how and the elbow grease t...\n",
       "2999    you think we could enrich each others lives in...\n",
       "Length: 6000, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays = df_okcupid['essay0'].append(df_okcupid['essay9']).fillna('')\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61  349     0.005385\n",
       "    1549    0.005344\n",
       "    1176    0.005336\n",
       "    2435    0.004986\n",
       "    605     0.004282\n",
       "    1301    0.004112\n",
       "    1925    0.003554\n",
       "    1963    0.003258\n",
       "    2466    0.003203\n",
       "    2       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(norm=None) # Do not normalize.\n",
    "vec.fit(essays) # This determines the vocabulary.\n",
    "#tf_idf_sparse = vec.transform(essays)\n",
    "#tf_idf_sparse\n",
    "tf_idf_sparse0 = pd.DataFrame.sparse.from_spmatrix(vec.transform(df_okcupid['essay0'].fillna('')))\n",
    "tf_idf_sparse0.index = df_okcupid['essay0'].index\n",
    "tf_idf_sparse9 = pd.DataFrame.sparse.from_spmatrix(vec.transform(df_okcupid['essay9'].fillna('')))\n",
    "tf_idf_sparse9.index = df_okcupid['essay9'].index\n",
    "tf_idf_sparse9.loc[[61]]\n",
    "\n",
    "CSM_okcupid = pd.DataFrame(cosine_similarity(tf_idf_sparse9.loc[[61]],tf_idf_sparse0),index=[61])#,columns=docs_seuss.index)\n",
    "CSM_okcupid.stack().drop_duplicates().sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61  2344    0.241772\n",
       "    2678    0.215790\n",
       "    959     0.212673\n",
       "    1936    0.211059\n",
       "    342     0.208499\n",
       "    862     0.208170\n",
       "    1859    0.204973\n",
       "    1117    0.202926\n",
       "    257     0.201132\n",
       "    224     0.199141\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSM_okcupid.stack().drop_duplicates().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9JUuovrSb76S"
   },
   "source": [
    "Exercises 4-5 ask you to work with the Enron spam data set (https://dlsun.github.io/pods/data/enron_spam.csv). This data set contains the subjects and bodies of a sample of e-mails that the Federal Energy Regulatory Commission (FERC) collected during their 2002 investigation of the energy company Enron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mirant 4 / 01</td>\n",
       "      <td>we invoiced mirant americas for deal 705989 an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>re : lobo payout</td>\n",
       "      <td>because the payback was done for october 2001 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entex transaction 7</td>\n",
       "      <td>for december 1999 , since the volumes for tran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>re : hpl transport contracts</td>\n",
       "      <td>i would think that the first contract should g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>welcome to aol instant messenger !</td>\n",
       "      <td>welcome to the aol instant messenger ( sm ) se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>NaN</td>\n",
       "      <td>discount meds right from home\\nvalium , xanax ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>reduce monthly - payments</td>\n",
       "      <td>thank you for your mor tg age application , wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>reply soon ! ! !</td>\n",
       "      <td>dear sir ,\\ni know this email will reach you a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>fwd : need | xianax ? vl @ gra % v + a + lium ...</td>\n",
       "      <td>we give you the power to make an educated choi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>new message .</td>\n",
       "      <td>you have [ 1 ] new message .\\nplease view it a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1990 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                subject  \\\n",
       "0                                         mirant 4 / 01   \n",
       "1                                      re : lobo payout   \n",
       "2                                   entex transaction 7   \n",
       "3                          re : hpl transport contracts   \n",
       "4                    welcome to aol instant messenger !   \n",
       "...                                                 ...   \n",
       "1985                                                NaN   \n",
       "1986                          reduce monthly - payments   \n",
       "1987                                   reply soon ! ! !   \n",
       "1988  fwd : need | xianax ? vl @ gra % v + a + lium ...   \n",
       "1989                                      new message .   \n",
       "\n",
       "                                                   body  spam  \n",
       "0     we invoiced mirant americas for deal 705989 an...     0  \n",
       "1     because the payback was done for october 2001 ...     0  \n",
       "2     for december 1999 , since the volumes for tran...     0  \n",
       "3     i would think that the first contract should g...     0  \n",
       "4     welcome to the aol instant messenger ( sm ) se...     0  \n",
       "...                                                 ...   ...  \n",
       "1985  discount meds right from home\\nvalium , xanax ...     1  \n",
       "1986  thank you for your mor tg age application , wh...     1  \n",
       "1987  dear sir ,\\ni know this email will reach you a...     1  \n",
       "1988  we give you the power to make an educated choi...     1  \n",
       "1989  you have [ 1 ] new message .\\nplease view it a...     1  \n",
       "\n",
       "[1990 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spam = pd.read_csv('https://dlsun.github.io/pods/data/enron_spam.csv')\n",
    "df_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UiqRT9Vxb76V"
   },
   "source": [
    "4\\. Each e-mail has additionally been manually labeled as spam (1) or not (0). Find the TF-IDF representation of the bodies of the e-mails. Which e-mail was most similar to e-mail 0 (not spam)? Which e-mail was most similar to e-mail 1001 (spam)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       we invoiced mirant americas for deal 705989 an...\n",
       "1       because the payback was done for october 2001 ...\n",
       "2       for december 1999 , since the volumes for tran...\n",
       "3       i would think that the first contract should g...\n",
       "4       welcome to the aol instant messenger ( sm ) se...\n",
       "                              ...                        \n",
       "1985    discount meds right from home\\nvalium , xanax ...\n",
       "1986    thank you for your mor tg age application , wh...\n",
       "1987    dear sir ,\\ni know this email will reach you a...\n",
       "1988    we give you the power to make an educated choi...\n",
       "1989    you have [ 1 ] new message .\\nplease view it a...\n",
       "Name: body, Length: 1990, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_spam['body'].fillna('')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001  1744    0.001125\n",
       "0     1852    0.000810\n",
       "1001  1728    0.000785\n",
       "      1842    0.000654\n",
       "      1644    0.000635\n",
       "      1505    0.000558\n",
       "      1973    0.000554\n",
       "0     1232    0.000297\n",
       "1001  1401    0.000229\n",
       "0     65      0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec = TfidfVectorizer(norm=None) # Do not normalize.\n",
    "vec.fit(corpus) # This determines the vocabulary.\n",
    "tf_idf_sparse = pd.DataFrame.sparse.from_spmatrix(vec.transform(corpus))\n",
    "\n",
    "CSM_spam = pd.DataFrame(cosine_similarity(tf_idf_sparse.loc[[0,1001]],tf_idf_sparse.drop([0,1001])),index=[0,1001])#,columns=docs_seuss.index)\n",
    "CSM_spam.stack().drop_duplicates().sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1663    0.573494\n",
       "1097    0.500739\n",
       "1120    0.448894\n",
       "1985    0.437058\n",
       "1010    0.427921\n",
       "          ...   \n",
       "1842    0.000654\n",
       "1644    0.000635\n",
       "1505    0.000558\n",
       "1973    0.000554\n",
       "1401    0.000229\n",
       "Length: 1695, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSM_spam.stack().drop_duplicates().sort_values(ascending=False).loc[1001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541     0.323298\n",
       "326     0.238129\n",
       "664     0.224681\n",
       "544     0.215916\n",
       "470     0.211796\n",
       "          ...   \n",
       "1644    0.001325\n",
       "1505    0.001298\n",
       "1852    0.000810\n",
       "1232    0.000297\n",
       "65      0.000000\n",
       "Length: 1737, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSM_spam.stack().drop_duplicates().sort_values(ascending=False).loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iy7hIjLDb76W"
   },
   "source": [
    "5\\. (This exercise requires material from Part II of this book.) Write a function `predict_spam()` that accepts an e-mail body and predicts whether or not it is spam using $k$-nearest neighbors on the Enron spam data set. Use cosine distance ($= 1 - \\text{cosine similarity}$) as your distance metric.\n",
    "\n",
    "Use your model to predict whether an e-mail with the body \"free cash\" is spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='cosine', n_neighbors=3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3,metric='cosine')\n",
    "neigh.fit(tf_idf_sparse, df_spam['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_spam(text):\n",
    "    return neigh.predict(vec.transform([text]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_spam('free cash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_spam('What did you have for dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_spam('What did free cash have for dinner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "10.2 The Vector Space Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
